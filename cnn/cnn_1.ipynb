{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.MAX_IMAGE_PIXELS = 933120000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to your data directories\n",
    "train_dir = 'EyeContact/train'\n",
    "validation_dir = 'EyeContact/validation'\n",
    "test_dir = 'EyeContact/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image dimensions and batch size\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 110 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create a data generator with data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 images belonging to 2 classes.\n",
      "Found 14 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create data generators for validation and test data (without data augmentation)\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model with pre-trained weights\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers of the VGG16 model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom layers for your specific classification task\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)  # Linear activation for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 196s 48s/step - loss: 1.0219 - accuracy: 0.4091 - val_loss: 0.6438 - val_accuracy: 0.6429\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 166s 48s/step - loss: 0.8181 - accuracy: 0.4545 - val_loss: 0.8731 - val_accuracy: 0.4286\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 182s 54s/step - loss: 0.7317 - accuracy: 0.5364 - val_loss: 0.6226 - val_accuracy: 0.6429\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 179s 45s/step - loss: 0.7620 - accuracy: 0.5182 - val_loss: 0.7007 - val_accuracy: 0.4286\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 179s 52s/step - loss: 0.6370 - accuracy: 0.6455 - val_loss: 0.7496 - val_accuracy: 0.4286\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 182s 42s/step - loss: 0.6789 - accuracy: 0.6455 - val_loss: 0.6369 - val_accuracy: 0.5714\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 174s 39s/step - loss: 0.7759 - accuracy: 0.4818 - val_loss: 0.6636 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 176s 42s/step - loss: 0.7567 - accuracy: 0.5818 - val_loss: 0.7225 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 181s 53s/step - loss: 0.7221 - accuracy: 0.5364 - val_loss: 0.6458 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 175s 51s/step - loss: 0.7504 - accuracy: 0.5273 - val_loss: 0.6288 - val_accuracy: 0.6429\n"
     ]
    }
   ],
   "source": [
    "# Create the final model\n",
    "model = Model(base_model.input, output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "history = model.fit(train_generator, epochs=epochs, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 21s 21s/step - loss: 0.7527 - accuracy: 0.2857\n",
      "Test Accuracy: 0.2857142984867096\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f'Test Accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('models/eye_contact_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have videos of job interviews and I want to build a convolutional neural network that can classify whether the interviewee is maintaining ample eye contact or not. My data directory contains two sub-directories: \"positive\" which contains videos where ample eye contact is being maintained and \"negative\" which contains videos where ample eye contact is not being maintained.\n",
    "\n",
    "Data preprocessing\n",
    "1. The first step in our pipeline is to extract frames from the video at regular intervals. We use the get_frames() function to extract a frame every 3 seconds. \n",
    "2. Next, we use the Haar Cascade algorithm [17] to detect faces in each frame. \n",
    "3. Once we have located the face, we use the save_cropped() function to crop the face and save it for further processing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
